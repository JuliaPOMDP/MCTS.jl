{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Domain Knowledge\n",
    "\n",
    "Aside from tuning the solver parameters (c, k, alpha), MCTS currently offers several means of incorporating domain knowledge. The following solver parameters control the planner's behavior:\n",
    "\n",
    "- `estimate_value` determines how the value is estimated at the leaf nodes (this is usually done using a rollout simulation).\n",
    "- `init_N` and `init_Q` determine how N(s,a) and Q(s,a) are initialized when a new node is created.\n",
    "- `next_action` determines which new actions are tried in double progressive widening\n",
    "\n",
    "There are three ways of specifying these parameters: 1) with constant values, 2) with functions, and 3) with custom objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MCTS\n",
    "using POMDPs\n",
    "using POMDPModels\n",
    "using Random\n",
    "mdp = SimpleGridWorld();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Values\n",
    "\n",
    "`init_N`, `init_Q`, and `estimate_value` can be set with constant values (though this is a bad idea for `estimate_value`. `next_action` cannot be specified in this way. The following code sets all new N to 3 and all new Q to 11.73 for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action Nodes\n",
      "s:[1, 1], a:up Q:7.037999999999999 N:5\n",
      "s:[1, 1], a:down Q:8.7975 N:4\n",
      "s:[1, 1], a:left Q:5.864999999999999 N:6\n",
      "s:[1, 1], a:right Q:11.73 N:3\n",
      "s:[1, 2], a:up Q:8.7975 N:4\n",
      "s:[1, 2], a:down Q:11.73 N:3\n",
      "s:[1, 2], a:left Q:11.73 N:3\n",
      "s:[1, 2], a:right Q:11.73 N:3\n",
      "s:[2, 2], a:up Q:11.73 N:3\n",
      "s:[2, 2], a:down Q:11.73 N:3\n",
      "s:[2, 2], a:left Q:11.73 N:3\n",
      "s:[2, 2], a:right Q:11.73 N:3\n",
      "s:[2, 1], a:up Q:11.73 N:3\n",
      "s:[2, 1], a:down Q:11.73 N:3\n",
      "s:[2, 1], a:left Q:11.73 N:3\n",
      "s:[2, 1], a:right Q:11.73 N:3\n"
     ]
    }
   ],
   "source": [
    "solver = MCTSSolver(n_iterations=3, depth=4,\n",
    "                    init_N=3,\n",
    "                    init_Q=11.73)\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GWPos(1,1))\n",
    "println(\"State-Action Nodes\")\n",
    "tree = policy.tree\n",
    "for sn in MCTS.state_nodes(tree)\n",
    "    for san in MCTS.children(sn)\n",
    "        println(\"s:$(MCTS.state(sn)), a:$(action(san)) Q:$(MCTS.q(san)) N:$(MCTS.n(san))\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "`init_N`, `init_Q`, `estimate_value`, and `next_action` can also be functions. The following code will\n",
    "\n",
    "- initialize Q to 0.0 everywhere except state [1,2] where it will be 11.73\n",
    "- initialize N to 0 everywhere except state [1,2] where it will be 3\n",
    "- estimate the value to be 10 divided by the manhattan distance to state [9,3]\n",
    "- always choose action \"up\" first in double progressive widening\n",
    "\n",
    "Note: the `?` below is part of the [ternary operator](http://docs.julialang.org/en/release-0.5/manual/control-flow/#control-flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_Q(mdp, s, a) = s == GWPos(1,2) ? 11.73 : 0.0\n",
    "special_N(mdp, s, a) = s == GWPos(1,2) ? 3 : 0\n",
    "\n",
    "function manhattan_value(mdp, s, remaining_depth) # note remaining depth is ignored\n",
    "    m_dist = abs(s.x-9)+abs(s.y-3)\n",
    "    val = 10.0/m_dist\n",
    "    println(\"Set value for $s to $val\") # this is not necessary - just shows that it's working later\n",
    "    return val\n",
    "end\n",
    "\n",
    "function up_priority(mdp, s, snode) # snode is the state node of type DPWStateNode\n",
    "    if haskey(snode.tree.a_lookup, (snode.index, :up)) # \"up\" is already there\n",
    "        return rand([:left, :down, :right]) # add a random action\n",
    "    else\n",
    "        return :up\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set value for [1, 2] to 1.1111111111111112\n",
      "Set value for [1, 3] to 1.25\n",
      "Set value for [2, 1] to 1.1111111111111112\n",
      "Set value for [1, 1] to 1.0\n",
      "Set value for [1, 1] to 1.0\n",
      "Set value for [2, 2] to 1.25\n",
      "Set value for [2, 3] to 1.4285714285714286\n",
      "Set value for [3, 1] to 1.25\n",
      "State-Action Nodes:\n",
      "s:[1, 1], a:up, Q:1.085133101851852 N:3\n",
      "s:[1, 1], a:right, Q:1.134156746031746 N:4\n",
      "s:[1, 1], a:left, Q:0.8810953125 N:4\n",
      "s:[1, 1], a:down, Q:0.8810953125 N:4\n",
      "s:[1, 2], a:up, Q:9.094375 N:4\n",
      "s:[2, 1], a:up, Q:1.2383928571428573 N:2\n",
      "s:[2, 1], a:right, Q:1.1875 N:1\n",
      "s:[2, 2], a:up, Q:1.3571428571428572 N:1\n"
     ]
    }
   ],
   "source": [
    "solver = DPWSolver(n_iterations=8, depth=4,\n",
    "                   init_N=special_N, init_Q=special_Q,\n",
    "                   estimate_value=manhattan_value,\n",
    "                   next_action=up_priority)\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GWPos(1,1))\n",
    "println(\"State-Action Nodes:\")\n",
    "tree = policy.tree\n",
    "for i in 1:length(tree.total_n)\n",
    "    for j in tree.children[i]\n",
    "        println(\"s:$(tree.s_labels[i]), a:$(tree.a_labels[j]), Q:$(tree.q[j]) N:$(tree.n[j])\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects\n",
    "\n",
    "There are many cases where functions are not suitable, for example when the solver needs to be serialized. In this case, arbitrary objects may be passed to the solver to encode the behavior. The same object can be passed to multiple solver parameters to govern all of their behavior. See the docstring for the solver for more information on which functions will be called on the object(s). The following code does exactly the same thing as the function-based code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct MyHeuristic\n",
    "    target_state::GWPos\n",
    "    special_state::GWPos\n",
    "    special_Q::Float64\n",
    "    special_N::Int\n",
    "    priority_action::Symbol\n",
    "    rng::AbstractRNG\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCTS.init_Q(h::MyHeuristic, mdp::SimpleGridWorld, s, a) = s == h.special_state ? h.special_Q : 0.0\n",
    "MCTS.init_N(h::MyHeuristic, mdp::SimpleGridWorld, s, a) = s == h.special_state ? h.special_N : 0\n",
    "\n",
    "function MCTS.estimate_value(h::MyHeuristic, mdp::SimpleGridWorld, s, remaining_depth)\n",
    "    targ = h.target_state\n",
    "    m_dist = abs(s.x-targ.x)+abs(s.y-targ.y)\n",
    "    val = 10.0/m_dist\n",
    "    println(\"Set value for $s to $val\") # this is not necessary - just shows that it's working later\n",
    "    return val\n",
    "end\n",
    "\n",
    "function MCTS.next_action(h::MyHeuristic, mdp::SimpleGridWorld, s, snode::DPWStateNode)\n",
    "    if haskey(snode.tree.a_lookup, (snode.index, h.priority_action))\n",
    "        return rand(h.rng, [:up, :left, :down, :right]) # add a random other action\n",
    "    else\n",
    "        return h.priority_action\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set value for [1, 2] to 1.1111111111111112\n",
      "Set value for [1, 3] to 1.25\n",
      "Set value for [2, 1] to 1.1111111111111112\n",
      "Set value for [1, 1] to 1.0\n",
      "Set value for [2, 2] to 1.25\n",
      "Set value for [1, 1] to 1.0\n",
      "Set value for [2, 3] to 1.4285714285714286\n",
      "Set value for [2, 1] to 1.1111111111111112\n",
      "State-Action Nodes:\n",
      "s:[1, 1], a:up, Q:0.9781366030092592 N:6\n",
      "s:[1, 1], a:right, Q:1.0663283482142856 N:5\n",
      "s:[1, 1], a:down, Q:0.8810953125 N:4\n",
      "s:[1, 2], a:up, Q:7.4898437499999995 N:5\n",
      "s:[1, 2], a:down, Q:9.0654296875 N:4\n",
      "s:[1, 2], a:left, Q:9.01184375 N:4\n",
      "s:[1, 2], a:right, Q:11.73 N:3\n",
      "s:[2, 1], a:up, Q:1.2383928571428573 N:2\n",
      "s:[2, 1], a:down, Q:1.0036574074074074 N:3\n",
      "s:[2, 2], a:up, Q:1.3571428571428572 N:1\n"
     ]
    }
   ],
   "source": [
    "heur = MyHeuristic(GWPos(9,3), GWPos(1,2), 11.73, 3, :up, Random.GLOBAL_RNG)\n",
    "solver = DPWSolver(n_iterations=8, depth=4,\n",
    "                   init_N=heur, init_Q=heur,\n",
    "                   estimate_value=heur,\n",
    "                   next_action=heur)\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GWPos(1,1))\n",
    "println(\"State-Action Nodes:\")\n",
    "tree = policy.tree\n",
    "for i in 1:length(tree.total_n)\n",
    "    for j in tree.children[i]\n",
    "        println(\"s:$(tree.s_labels[i]), a:$(tree.a_labels[j]), Q:$(tree.q[j]) N:$(tree.n[j])\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollouts\n",
    "\n",
    "The most common way to estimate the value of a state node is with rollout simulations. This can be done with an arbitrary policy or solver by passing a `RolloutEstimator` object as the `estimate_value` parameter. The following code does this with a policy that moves towards state [9,3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct SeekTarget <: Policy\n",
    "    target::GWPos\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.action(p::SeekTarget, s::GWPos, a::Symbol=:up)\n",
    "    if p.target.x > s.x\n",
    "        return :right\n",
    "    elseif p.target.x < s.x\n",
    "        return :left\n",
    "    elseif p.target.y > s.y\n",
    "        return :up\n",
    "    else\n",
    "        return :down\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action Nodes\n",
      "s:[5, 1], a:up Q:-1.2931903038081076 N:2\n",
      "s:[5, 1], a:down Q:6.634204312890622 N:1\n",
      "s:[5, 1], a:left Q:4.401266686517653 N:1\n",
      "s:[5, 1], a:right Q:5.987369392383786 N:1\n",
      "s:[4, 1], a:up Q:4.632912301597529 N:1\n",
      "s:[4, 1], a:down Q:0.0 N:0\n",
      "s:[4, 1], a:left Q:0.0 N:0\n",
      "s:[4, 1], a:right Q:0.0 N:0\n",
      "s:[5, 2], a:up Q:0.0 N:0\n",
      "s:[5, 2], a:down Q:0.0 N:0\n",
      "s:[5, 2], a:left Q:0.0 N:0\n",
      "s:[5, 2], a:right Q:0.0 N:0\n",
      "s:[6, 1], a:up Q:6.3024940972460906 N:1\n",
      "s:[6, 1], a:down Q:0.0 N:0\n",
      "s:[6, 1], a:left Q:0.0 N:0\n",
      "s:[6, 1], a:right Q:0.0 N:0\n",
      "s:[4, 2], a:up Q:0.0 N:0\n",
      "s:[4, 2], a:down Q:0.0 N:0\n",
      "s:[4, 2], a:left Q:0.0 N:0\n",
      "s:[4, 2], a:right Q:0.0 N:0\n",
      "s:[6, 2], a:up Q:0.0 N:0\n",
      "s:[6, 2], a:down Q:0.0 N:0\n",
      "s:[6, 2], a:left Q:0.0 N:0\n",
      "s:[6, 2], a:right Q:0.0 N:0\n"
     ]
    }
   ],
   "source": [
    "solver = MCTSSolver(n_iterations=5, depth=20,\n",
    "                    estimate_value=RolloutEstimator(SeekTarget(GWPos(9,3)); max_depth=50))\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GWPos(5,1))\n",
    "println(\"State-Action Nodes\")\n",
    "tree = policy.tree\n",
    "for sn in MCTS.state_nodes(tree)\n",
    "    for san in MCTS.children(sn)\n",
    "        println(\"s:$(MCTS.state(sn)), a:$(action(san)) Q:$(MCTS.q(san)) N:$(MCTS.n(san))\")\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
